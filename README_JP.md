# チーフインテリジェンスオフィサー (Wiseflow)

**[中文](README_CN.md) | [English](README.md) | [Français](README_FR.md) | [Deutsch](README_DE.md)**

**チーフインテリジェンスオフィサー** (Wiseflow) は、ウェブサイト、WeChat公式アカウント、ソーシャルメディアなどのさまざまな情報源から、事前に設定された関心点に基づいて情報を抽出し、自動的にタグ付けしてデータベースにアップロードすることができるアジャイルな情報抽出ツールです。

---

SiliconFlowは、Qwen2-7B-Instructやglm-4-9b-chatなどのいくつかのLLMオンライン推論サービスが2024年6月25日から無料で提供されることを正式に発表しました。これにより、「ゼロコスト」でwiseflowを使って情報発掘が可能になります！

---

私たちが必要なのは情報ではなく、膨大な情報の中からノイズを取り除き、価値のある情報を浮き彫りにすることです！ 

チーフインテリジェンスオフィサーがどのようにして時間を節約し、無関係な情報をフィルタリングし、注目すべきポイントを整理するのかをご覧ください！

https://github.com/TeamWiseFlow/wiseflow/assets/96130569/bd4b2091-c02d-4457-9ec6-c072d8ddfb16

<img alt="sample.png" src="asset/sample.png" width="1024"/>

## 🔥 V0.3.0 重要なアップデート

- ✅ GNE（オープンソースプロジェクト）とLLMを使用して再構築した新しい汎用ウェブページコンテンツパーサー。90%以上のニュースページに適応可能。

- ✅ 新しい非同期タスクアーキテクチャ。

- ✅ 新しい情報抽出とタグ分類戦略。より正確で繊細な情報を提供し、9BサイズのLLMのみで完璧にタスクを実行します。

## 🌟 主な機能

- 🚀 **ネイティブ LLM アプリケーション**  
  コストを最大限に抑え、データセンシティブなユーザーがいつでも完全にローカルデプロイに切り替えられるよう、最適な7B~9Bオープンソースモデルを慎重に選定しました。


- 🌱 **軽量設計**  
  ベクトルモデルを使用せず、システム負荷が小さく、GPU不要であらゆるハードウェア環境に対応します。


- 🗃️ **インテリジェントな情報抽出と分類**  
  様々な情報源から自動的に情報を抽出し、ユーザーの関心に基づいてタグ付けと分類を行います。

  😄 **Wiseflowは特にWeChat公式アカウントの記事から情報を抽出するのが得意です**。そのため、mp記事専用パーサーを設定しました！


- 🌍 **任意のAgentプロジェクトに統合可能**  
  任意のAgentプロジェクトの動的ナレッジベースとして機能し、Wiseflowのコードを理解せずとも、データベースからの読み取り操作だけで利用できます！


- 📦 **人気のPocketBaseデータベース**  
  データベースとインターフェースにPocketBaseを使用。Webインターフェースに加え、Go/JavaScript/PythonなどのSDKもあります。
    
    - Go: https://pocketbase.io/docs/go-overview/
    - JavaScript: https://pocketbase.io/docs/js-overview/
    - Python: https://github.com/vaphes/pocketbase

## 🔄 Wiseflowと一般的なクローラツール、LLM-Agentプロジェクトとの違いと関連性

| 特徴          | チーフインテリジェンスオフィサー (Wiseflow) | クローラ / スクレイパー                     | LLM-Agent                 |
|---------------|---------------------------------|------------------------------------------|---------------------------|
| **解決する主な問題** | データ処理（フィルタリング、抽出、タグ付け） | 生データの取得                             | 下流アプリケーション                |
| **関連性**     |                                | Wiseflowに統合して、より強力な生データ取得能力を持たせる | 動的ナレッジベースとしてWiseflowを統合可能 |

## 📥 インストールと使用方法

チーフインテリジェンスオフィサーはハードウェアの要件がほとんどなく、システム負荷が小さく、GPUやCUDAを必要としません（オンラインLLMサービスを使用する場合）。

1. **リポジトリをクローン**

     😄 いいねやforkは良い習慣です

    ```bash
    git clone https://github.com/TeamWiseFlow/wiseflow.git
    cd wiseflow
    ```

2. **Dockerの使用を強く推奨**

    ```bash
    docker compose up
    ```
   必要に応じて`compose.yaml`を変更できます。

    **注意:**
    - 上記のコマンドはwiseflowリポジトリのルートディレクトリで実行してください。
    - 実行前に`.env`ファイルを作成し、Dockerfileと同じディレクトリ（wiseflowリポジトリのルートディレクトリ）に配置します。.envファイルの参考例は`env_sample`です。
    - 最初にDockerコンテナを実行するとエラーが発生する可能性があります。これは正常で、まだpbリポジトリにadminアカウントを作成していないためです。

    この場合、コンテナを閉じずに、ブラウザで`http://127.0.0.1:8090/_/`を開き、指示に従ってadminアカウント（必ずメールアドレスを使用してください）を作成し、作成したadminメールアドレス（再度、必ずメールアドレスを使用してください）とパスワードを.envファイルに記入してコンテナを再起動してください。

    _コンテナのタイムゾーンと言語を変更したい場合 [プロンプトの言語を決定しますが、結果にはあまり影響しません] は、以下のコマンドでイメージを実行してください_

    ```bash
    docker run -e LANG=ja_JP.UTF-8 -e LC_CTYPE=ja_JP.UTF-8 your_image
    ```

3. **【代替】Pythonで直接実行**

    ```bash
    conda create -n wiseflow python=3.10
    conda activate wiseflow
    cd core
    pip install -r requirements.txt
    ```

    その後、core/scriptsにあるスクリプトでpb、task、backendを個別に起動できます（スクリプトファイルをcoreディレクトリに移動）。

    注意:
    - pbを最初に起動してください。taskとbackendは独立したプロセスなので、順番は問いません。また、必要に応じてどれか一つだけを起動することもできます。
    - 自分のデバイスに適したpocketbaseクライアントをhttps://pocketbase.io/docs/ からダウンロードし、/core/pbディレクトリに配置してください。
    - pbに関する問題（初回実行時のエラーなど）は [core/pb/README.md](/core/pb/README.md) を参照してください。
    - 使用前に`.env`ファイルを作成して編集し、wiseflowリポジトリのルートディレクトリ（coreディレクトリの上位）に配置します。.envファイルの参考例は`env_sample`で、詳細な設定については以下を参照してください。


    📚 開発者向けには [/core/README.md](/core/README.md) を参照してください。

        pocketbaseでデータにアクセス:
        - http://127.0.0.1:8090/_/ - 管理ダッシュボードUI
        - http://127.0.0.1:8090/api/ - REST API


4. **設定**

    `env_sample`をコピーし、`.env`に名前を変更してから、以下のように設定情報（LLMサービスのトークンなど）を記入します。

   - LLM_API_KEY # 大規模言語モデル推論サービスのAPIキー
   - LLM_API_BASE # このプロジェクトはOpenAI SDKに依存しています。モデルサービスがOpenAI APIをサポートしている場合、この項目を設定することで正常に使用できます。OpenAIサービスを使用する場合はこの項目を削除してください。
   - WS_LOG="verbose"  # デバッグ観察を開始するかどうかの設定。必要がない場合は削除してください。
   - GET_INFO_MODEL # 情報抽出とタグマッチングタスクモデル。デフォルトは gpt-3.5-turbo
   - REWRITE_MODEL # 近似情報の統合と書き換えタスクモデル。デフォルトは gpt-3.5-turbo
   - HTML_PARSE_MODEL # ウェブページ解析モデル（GNEアルゴリズムがうまく機能しない場合に自動的に使用）。デフォルトは gpt-3.5-turbo
   - PROJECT_DIR # データ、キャッシュ、およびログファイルの保存場所。リポジトリに対する相対パス。デフォルトではリポジトリ内。
   - PB_API_AUTH='email|password' # pbデータベースのadminのメールアドレスとパスワード（必ずメールアドレスで、仮のメールアドレスでも可）
   - PB_API_BASE  # 通常は不要。この項目は、デフォルトのポケットベースローカルインターフェース（8090）を使用しない場合にのみ設定してください。


5. **モデルの推奨**

    多くのテスト（中国語と英語のタスク向け）に基づき、**GET_INFO_MODEL**、**REWRITE_MODEL**、**HTML_PARSE_MODEL** の3項目に対してそれぞれ **"zhipuai/glm4-9B-chat"**、**"alibaba/Qwen2-7B-Instruct"**、**"alibaba/Qwen2-7B-Instruct"** を推奨します。

    これらは本プロジェクトに非常に適しており、指示の遵守が安定していて、生成結果が優れています。本プロジェクトに関連するプロンプトもこれらの3つのモデルに最適化されています。（**HTML_PARSE_MODEL**は **"01-ai/Yi-1.5-9B-Chat"** も使用可能で、非常に良好な結果を示しています。）


    ⚠️ **SiliconFlow**のオンライン推論サービスの使用も強くお勧めします。低価格、高速、無料枠が多い！⚠️

    SiliconFlowのオンライン推論サービスはOpenAI SDKと互換性があり、上記3つのモデルのオープンソースサービスも提供しています。`LLM_API_BASE`を "https://api.siliconflow.cn/v1" に設定し、`LLM_API_KEY`を設定するだけで利用できます。

    😄 または、私の[招待リンク](https://cloud.siliconflow.cn?referrer=clx6wrtca00045766ahvexw92)を使用して、私がより多くのトークンを得られるようにすることもできます 😄


6. **注目点と定期スキャンソースの追加**

    プログラムを起動した後、pocketbaseの管理ダッシュボードUI (http://127.0.0.1:8090/_/) を開きます。

        6.1 **tagsフォーム**を開く

        このフォームで関心事項を指定できます。LLMはこれに基づいて情報を抽出、フィルタリング、分類します。

        tagsフィールドの説明:

        - name, 関心事項の説明。**注意: より具体的にしてください。** 良い例: `米中競争動向`。悪い例: `国際情勢`。
        - activated, アクティブかどうか。無効にするとその関心事項は無視されます。無効化後に再度有効化できます。有効化と無効化にはDockerコンテナの再起動は不要で、次回の定期タスク時に更新されます。

        6.2 **sitesフォーム**を開く

        このフォームでカスタムソースを指定できます。システムはバックグラウンドで定期タスクを起動し、これらのソースをローカルでスキャン、解析、分析します。

        sitesフィールドの説明:

        - url, ソースのURL。特定の記事ページではなく、記事リストのページを指定してください。
        - per_hours, スキャン頻度。単位は時間。整数（1〜24範囲で、1日1回を超えない頻度（24を設定）を推奨）。
        - activated, アクティブかどうか。無効にするとそのソースは無視されます。無効化後に再度有効化できます。有効化と無効化にはDockerコンテナの再起動は不要で、次回の定期タスク時に更新されます。


7. **ローカルデプロイ**

    ご覧の通り、本プロジェクトは7B/9BのLLMを使用しており、ベクトルモデルを必要としません。つまり、RTX 3090（24 GB VRAM）1台で完全にローカルデプロイできます。

    お使いのローカルLLMサービスがOpenAI SDKと互換性があることを確認し、`LLM_API_BASE`を適切に設定してください。



## 🛡️ ライセンス

このプロジェクトは [Apache 2.0](LICENSE) ライセンスの下でオープンソースです。

商用利用やカスタマイズの協力については、**メール: 35252986@qq.com** までご連絡ください。

- 商用顧客の方は、登録をお願いします。この製品は永久に無料であることをお約束します。
- カスタマイズが必要な顧客のために、ソースとビジネスニーズに応じて以下のサービスを提供します:
  - お客様のビジネスシーンソース用の専用クローラーとパーサー
  - カスタマイズされた情報抽出と分類戦略
  - 特定の LLM 推奨または微調整サービス
  - プライベートデプロイメントサービス
  - UI インターフェースのカスタマイズ

## 📬 お問い合わせ情報

ご質問やご提案がありましたら、[issue](https://github.com/TeamWiseFlow/wiseflow/issues) を通じてお気軽にお問い合わせください。

## 🤝 このプロジェクトは以下の優れたオープンソースプロジェクトに基づいています:

- GeneralNewsExtractor (統計学習に基づくニュースウェブページ本文の一般抽出器) https://github.com/GeneralNewsExtractor/GeneralNewsExtractor
- json_repair (無効な JSON ドキュメントの修復) https://github.com/josdejong/jsonrepair/tree/main 
- python-pocketbase (Python 用 PocketBase クライアント SDK) https://github.com/vaphes/pocketbase

# 引用

このプロジェクトの一部または全部を関連する作業で参照または引用する場合は、以下の情報を明記してください:

```
Author: Wiseflow Team
https://openi.pcl.ac.cn/wiseflow/wiseflow
https://github.com/TeamWiseFlow/wiseflow
Licensed under Apache2.0
```